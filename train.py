import os
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from datasets import load_dataset
from torchvision import transforms
from tqdm import tqdm
import glob
from nbb import UpsampleConcatClassifier  # ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ import

# -----------------------------------
# ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (ì—†ìœ¼ë©´ None)
# -----------------------------------
# PRETRAINED_MODEL_PATH = "checkpoints/best_model_epoch6.pth"  # ë˜ëŠ” None
PRETRAINED_MODEL_PATH = None  # ë˜ëŠ” None

if __name__ == "__main__":
    # ----------------------------
    # ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬
    # ----------------------------
    train_ds = load_dataset("food101", split="train")
    val_ds = load_dataset("food101", split="validation")


    def transform_example(example):
        from PIL import Image
        try:
            image = example["image"].convert("RGB")
        except Exception:
            return None

        transform = transforms.Compose([
            transforms.Resize((320, 320)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225]),
        ])
        example["pixel_values"] = transform(image)
        return example


    train_ds = train_ds.map(transform_example, num_proc=1)
    val_ds = val_ds.map(transform_example, num_proc=1)
    train_ds.set_format(type='torch', columns=['pixel_values', 'label'])
    val_ds.set_format(type='torch', columns=['pixel_values', 'label'])

    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=4)

    # ----------------------------
    # ëª¨ë¸/ì˜µí‹°ë§ˆì´ì €/ë¡œìŠ¤ ì„¤ì •
    # ----------------------------
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = UpsampleConcatClassifier(num_classes=101).to(device)

    # âœ… ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    if PRETRAINED_MODEL_PATH is not None and os.path.exists(PRETRAINED_MODEL_PATH):
        model.load_state_dict(torch.load(PRETRAINED_MODEL_PATH, map_location=device))
        print(f"âœ… Loaded pretrained model from {PRETRAINED_MODEL_PATH}")

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)


    # ----------------------------
    # í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜ ì •ì˜
    # ----------------------------
    def train_epoch(model, dataloader, criterion, optimizer, device):
        model.train()
        total_loss, total_correct, total_samples = 0, 0, 0

        pbar = tqdm(dataloader, desc="Training", leave=False)
        for batch in pbar:
            inputs = batch['pixel_values'].to(device)
            labels = batch['label'].to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            preds = outputs.argmax(dim=1)
            total_loss += loss.item() * inputs.size(0)
            total_correct += (preds == labels).sum().item()
            total_samples += inputs.size(0)

            pbar.set_postfix({
                "Batch Loss": f"{loss.item():.4f}",
                "Avg Acc": f"{(total_correct / total_samples):.4f}"
            })

        avg_loss = total_loss / total_samples
        accuracy = total_correct / total_samples
        return avg_loss, accuracy


    def eval_epoch(model, dataloader, criterion, device):
        model.eval()
        total_loss, total_correct, total_samples = 0, 0, 0
        start_time = time.perf_counter()

        with torch.no_grad():
            pbar = tqdm(dataloader, desc="Validating", leave=False)
            for batch in pbar:
                inputs = batch['pixel_values'].to(device)
                labels = batch['label'].to(device)

                outputs = model(inputs)
                loss = criterion(outputs, labels)
                preds = outputs.argmax(dim=1)

                total_loss += loss.item() * inputs.size(0)
                total_correct += (preds == labels).sum().item()
                total_samples += inputs.size(0)

                pbar.set_postfix({
                    "Batch Loss": f"{loss.item():.4f}",
                    "Avg Acc": f"{(total_correct / total_samples):.4f}"
                })

        end_time = time.perf_counter()
        inference_time = end_time - start_time
        avg_inference_time = inference_time / total_samples

        avg_loss = total_loss / total_samples
        accuracy = total_correct / total_samples
        return avg_loss, accuracy, inference_time, avg_inference_time


    # ----------------------------
    # ì „ì²´ í•™ìŠµ ì‹¤í–‰ + ëª¨ë¸ ì €ì¥
    # ----------------------------
    os.makedirs("checkpoints", exist_ok=True)
    best_val_acc = 0.0
    epoch = 0
    prev_train_acc = 0.0
    prev_val_acc = 0.0

    while True:
        epoch += 1
        print(f"\n===== Epoch {epoch} =====")
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
        val_loss, val_acc, val_time, avg_inf_time = eval_epoch(model, val_loader, criterion, device)

        print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
        print(f"Val   Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | "
              f"Inference Time: {val_time:.2f}s | Avg Per Image: {avg_inf_time * 1000:.2f}ms")

        # âœ… ì—í¬í¬ë³„ ëª¨ë¸ ì €ì¥
        if epoch % 2 == 0:
            new_path = (
                f"checkpoints/epoch{epoch}_"
                f"train{train_acc * 100:.2f}_val{val_acc * 100:.2f}.pth"
            )
            torch.save(model.state_dict(), new_path)

        # âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_epoch = epoch

            # ê¸°ì¡´ best ëª¨ë¸ ì‚­ì œ
            for f in glob.glob("checkpoints/best_*.pth"):
                os.remove(f)

            # ìƒˆ íŒŒì¼ëª… ìƒì„±
            new_best_path = (
                f"checkpoints/best_epoch{epoch}_"
                f"train{train_acc * 100:.2f}_val{val_acc * 100:.2f}.pth"
            )

            torch.save(model.state_dict(), new_best_path)
            print(f"ğŸ’¾ Saved best model as {os.path.basename(new_best_path)}.")

        # ìƒíƒœ ê°±ì‹ 
        prev_train_acc = train_acc
        prev_val_acc = val_acc
